{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15b0e30-6eed-46b7-98b9-dfc3fa116b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "    .builder \n",
    "    .appName(\"Triggers in Spark Streaming\") \n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')\n",
    "    .config(\"spark.sql.shuffle.partitions\", 2)\n",
    "    .master(\"local[*]\") \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd26ea7-520d-4b70-bcb8-bd2937abbe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import count, lit, window, avg, stddev, sum, pow\n",
    "        \n",
    "# Create the kafka_df to read from kafka\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")\n",
    "    .option(\"subscribe\", \"MatchTrades-data,Lob-data\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert binary to string value column\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "kafka_json_df = kafka_df.withColumn(\"value\", expr(\"cast(value as string)\"))\n",
    "\n",
    "from pyspark.sql.functions import from_json, col, split, explode, window\n",
    "\n",
    "# JSON Schema\n",
    "json_schema = \"date timestamp, spread string, LDispersion string, price string, topic string\"\n",
    "\n",
    "# Expand JSON from Value column using Schema\n",
    "json_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema))\n",
    "\n",
    "# Select the required columns\n",
    "flattened_df = json_df.select(\"values_json.date\",\"values_json.price\",\"values_json.spread\",\"values_json.LDispersion\",\"topic\")\n",
    "\n",
    "# Split the data in words\n",
    "data_df = flattened_df \\\n",
    "    .withColumn(\"spread\", col(\"spread\")) \\\n",
    "    .withColumn(\"LDispersion\", col(\"LDispersion\")) \\\n",
    "    .withColumn(\"price\", col(\"price\")) \\\n",
    "    .withColumn(\"topic\",col(\"topic\")) \\\n",
    "    .withColumn(\"date\", col(\"date\").cast(\"timestamp\")) \\\n",
    "    \n",
    "# Aggregate the words to generate count\n",
    "from pyspark.sql.functions import count, lit, window, avg, stddev\n",
    "\n",
    "df_agg = data_df \\\n",
    "    .withWatermark(\"date\", \"30 minutes\") \\\n",
    "    .groupBy(window(\"date\", \"10 minutes\",\"5 minutes\")) \\\n",
    "    .agg(avg(\"price\").alias(\"avg_price\"),stddev(\"price\").alias(\"stddev_price\")) \n",
    "\n",
    "df_final = df_agg.selectExpr(\"window.start as start_time\", \"window.end as end_time\",\"avg_price\",\"stddev_price\") \\\n",
    "#.orderBy(['start_time','end_time'], ascending = [False,False])\n",
    "\n",
    "result = (df_final.selectExpr(\n",
    "                    \"CAST(start_time AS STRING)\",\n",
    "                    \"CAST(end_time AS STRING)\",\n",
    "                    \"CAST(avg_price AS STRING)\",\n",
    "                    \"CAST(stddev_price AS STRING)\",\n",
    "            ).withColumn(\"value\", to_json(struct(\"*\")).cast(\"string\"),)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda52244-5754-4018-9054-f865eacbf3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "(result\n",
    ".select(\"value\")\n",
    ".writeStream.trigger(processingTime=\"10 seconds\")\n",
    ".outputMode(\"update\")\n",
    ".format(\"kafka\")\n",
    ".option(\"topic\", \"sliding_window\")\n",
    ".option(\"kafka.bootstrap.servers\",  \"ed-kafka:29092\")\n",
    ".option(\"checkpointLocation\", \"sliding_window\")\n",
    ".start()\n",
    ".awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e5d8b-18e4-441e-b12c-887944be5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "(result\n",
    " .writeStream\n",
    " .format(\"console\")\n",
    " .outputMode(\"update\")\n",
    " .trigger(processingTime='10 seconds')\n",
    " .option(\"checkpointLocation\", \"checkpoint stream 2\")\n",
    " .start()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
